---
title: "ch9_lab"
author: "Christoper Chan"
date: "March 1, 2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(e1071)
library(caret)
```

### 9.6.1
Setting up our randomly generated data.
```{r}
set.seed(1)

x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1, 10), rep(1, 10))

x[y==1,] <- x[y==1,] + 1
plot(x, col=(3-y))
```

Creating a dataframe from our data and running a SVM on it.
```{r}
df <- data.frame(x=x, y=as.factor(y))

svm_fit <- svm(y~., df, kernel='linear', cost=10, scale=FALSE)
summary(svm_fit)
plot(svm_fit, df)
```

Observations in the margins.
```{r}
svm_fit$index
```

Running SVM again with a smaller cost parameter.
```{r}
svm_fit_lc <- svm(y~., df, kernel='linear', cost=0.1, scale=FALSE)

summary(svm_fit_lc)
plot(svm_fit_lc, df)
```

tune() preforms 10 fold CV on a model.
```{r}
set.seed(1)

tune_svm <- tune(svm, y~., data=df, kernel='linear', ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))

summary(tune_svm)
```

tune() stores the model with the lowest error in a variable called best.model
```{r}
best_model <- tune_svm$best.model
summary(best_model)
```
Testing the accuracy of our SVC model
```{r}
x_test <- matrix(rnorm(20*2), ncol=2)
y_test <- sample(c(-1,1), 20, rep=TRUE)
x_test[y_test==1,] <- x_test[y_test==1,] + 1
test_data <- data.frame(x=x_test, y=as.factor(y_test))
```

```{r}
y_pred <- predict(best_model, test_data)

confusionMatrix(y_pred, test_data$y)
```

Running svm with cost=0.01. In this case it preforms far worse, with a accuracy of 0.7 and a much lower specificity.
```{r}
svm_fit_0.01 <- svm(y~., df, kernel='linear', cost=0.01, scale=FALSE)

y_pred_0.01 <- predict(svm_fit_0.01, test_data)
confusionMatrix(y_pred_0.01, test_data$y)
```

Classes are linearlly seperable.
```{r}
x[y==1,] <- x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
```

Creating a SVC that linearly seperates the 2 classes. Very few observations, 3, are used in creating the hyperplane. There is a very narrow margin, which means it might perform poorly on test data. Let's see.
```{r}
df_linear <- data.frame(x=x, y=as.factor(y))

svm_fit_linear <- svm(y~., df_linear, kernel='linear', cost=1e5)

summary(svm_fit_linear)
plot(svm_fit_linear, df_linear)
```

To generalize model above we'll use a much lower cost. This time 5 observations are used as support vectors and we have a much wider margin.
```{r}
svm_fit_linear <- svm(y~., df_linear, kernel='linear', cost=1)
summary(svm_fit_linear)
plot(svm_fit_linear, df_linear)
```

### 9.6.2
Non-linear data
```{r}
set.seed(1)

x <- matrix(rnorm(200*2), ncol=2)
x[1:100,] <- x[1:100,]+2
x[101:150,] <- x[101:150,]-2
y <- c(rep(1,150), rep(2,50))

df <- data.frame(x=x, y=as.factor(y))
plot(x, col=y, pch=19)
```

Creating our radial SVM model
```{r}
# Randomly sampling for obs that'll be our training dataset 
train <- sample(200, 100)

svm_fit <- svm(y~., df[train,], kernel='radial', gamma=1, cost=1)
plot(svm_fit, df[train,])
```

Using the same radial kernel, but greatly increasing the cost function. It provides a much more complex decision boundary and probably overfits the data.
```{r}
svm_fit <- svm(y~., df[train,], kernel='radial', gamma=1, cost=1e5)

plot(svm_fit, df[train,])
```

Tuning the hyperparameters of cost and $\gamma$ using tune, which performs soft-margin grid search of hyperparameter space. tune() gives the optimal hyperparameters as cost=1 and $\gamma=0.5$.
```{r}
set.seed(1)

tune_out <- tune(svm, y~., data=df[train,], kernel='radial', ranges=list(cost=c(0.1,1,10,100,1000), gamma=c(0.5,1,2,3,4)))
summary(tune_out)
```

testing with optimal parameters. We had a accuracy of 88%, which isn't too bad.
```{r}
predicted <- predict(tune_out$best.model, newdata=df[-train,])
confusionMatrix(predicted, df[-train, 'y'])
```
















