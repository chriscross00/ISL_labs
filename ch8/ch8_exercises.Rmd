---
title: "ch8_exercises"
author: "Christoper Chan"
date: "January 6, 2019"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(gbm)
library(tree)
library(randomForest)
library(ISLR)
library(MASS)
```

MSE function
```{r}
MSE <- function(x,y){
    mean((x - y)^2)
}
```

##Applied
###7
```{r}
rf_bos <- randomForest(medv~., Boston, ntree=500, mtry=13, importance=T)
plot(rf_bos)
```

So I tried for a good 45 minutes to do this in a for loop and as a function but no luck. The output of randomForest can't be stored in a data.frame. 

TO DO:
Create training and test dataset.
Create different mtry, m=p, m=p/2, m=sqrt(p)



FINALLY! I did it with a for loop. My mistake before was i was putting the mse df in the for loop so the df was being reset each time.
```{r}
tries <- 500

mse <- data.frame(matrix(NA, nrow=tries, ncol=13))
for (i in 1:13){
    a <- randomForest(medv~., Boston, ntree=tries, mtry=i, importance=T)
    mse[i] <- a$mse
}

mse$ntree <- 1:tries
head(mse)
```

Creates a graph of MSE for each mtry. 
```{r}
long <- gather(mse, mtry, value, -ntree)
head(long)


ggplot(long, aes(ntree, value, color=mtry)) +
    geom_line() +
    ylab('MSE')
```
MSE seems lo level off after 150 trees. X1 has the highest MSE of the mtry while X9 has the lowest minimum MSE.

df conversion done with the reshape2 package.
```{r, results='hide'}
library(reshape2)
df <- melt(mse, id.vars='ntree', variable.names='mtry')

View(df)

ggplot(df, aes(ntree, value)) +
    geom_line(aes(color=variable))
```


###8
(a) Training data is 3/4 of the Carseats data.
```{r}
set.seed(5)

train <- sample(1:nrow(Carseats), (3/4)*nrow(Carseats))
test <- slice(Carseats, -train)
train <- Carseats[train,]
```

(b)
```{r}
reg_tree <- tree(Sales~., train)
summary(reg_tree)
```

Plot of tree
```{r}
plot(reg_tree)
text(reg_tree, cex=0.8, pretty=0)
```

MSE is around 4.57.
```{r}
pred_carseats <- predict(reg_tree, test)
MSE(pred_carseats, test$Sales)
```

(c) Looks like the best tree size is 9.
```{r}
set.seed(10)

cv_tree <- cv.tree(reg_tree, FUN=prune.tree)
cv_tree

par(mfrow=c(1,2))
plot(cv_tree$size, cv_tree$dev, type='b')
plot(cv_tree$k, cv_tree$dev, type='b')
```

Running prune.tree with best=9
```{r}
pruned <- prune.tree(reg_tree, best=9)

plot(pruned)
text(pruned, pretty=0)
```

Calculating MSE of pruned tree. We get a MSE of 3.74, which is roughly 0.8 lower than a reg tree. Pruning the tree does decrease the MSE.
```{r}
pred_pruned <- predict(pruned, test)
MSE(pred_pruned, test$Sales)
```

(d) So I just need to remeber that I need to use the randomForest() where mtry = $p$ in order to only do bagging.
```{r}
set.seed(1)

bag_car <- randomForest(Sales~., train, mtry=10, importance=TRUE)
bag_car
importance(bag_car)
```

We get a test MSE of 1.93. Which is lower than the training MSE of 2.65. Price, ShelveLoc & CompPrice are the most importance variables.
```{r}
pred_bag <- predict(bag_car, test)

MSE(pred_bag, test$Sales)
```

(e)
```{r}
set.seed(2)

rf_car2 <- randomForest(Sales~., train, mtry=5, importance=T)
rf_car2
importance(rf_car2)
```

```{r}
pred_rf2 <- predict(rf_car2, test)
MSE(pred_rf2, test$Sales)
```

```{r}
rf_carsqrt <- randomForest(Sales~., train, mtry=sqrt(10), importance=T)

rf_carsqrt
importance(rf_carsqrt)
```

Random forest was run twice, once with mtry=$p/2$ and again with mtry=$\sqrt{p}$. The MSE of $p/2$ is 2.05 and the MSE of $\sqrt{p}$ is 2.46. Both are a increase in MSE over bagging. Both rf show very similar importance, with Price and Shelveloc being the most importance predictors for both, but then $p/2$ chooses CompPrice as the third most important predictor while $sqrt{p}$ chooses Age.  
```{r}
pred_rfsqrt <- predict(rf_carsqrt, test)
MSE(pred_rfsqrt, test$Sales)
```


###9
(a) Training dataset of 800 observations
```{r, message=FALSE}
set.seed(20)

train <- sample(nrow(OJ), 800)
test <- slice(OJ, -train)
train <- OJ[train,]
```

(b) We get 9 terminal nodes. The tree is built from only 4 varibles: LoyalCH, PriceDiff, DiscMM, Store7. The training error rate is 0.1588.
```{r}
oj_tree <- tree(Purchase~., train)
summary(oj_tree)
```

```{r}
plot(oj_tree)
text(oj_tree, pretty=0)
```

(c) Node 10, which is a terminal node, classifies a MM. If the price difference is less than $0.17 than customer will pick MM over CH. In this case, 56 customers are in this terminal node with a deviance of 58.19. 21.43% of Purchase have CH as the value while remaining 78.57% of Purchase have the value MM.
```{r}
oj_tree
```

(d) See (a)
(e) The test error is `{r}(132+85)/(132+27+26+85)`
```{r}
oj_tpred <- predict(oj_tree, test, type='class')

table(oj_tpred, test$Purchase)
```


(f)
```{r}
oj_cv_tree <- cv.tree(oj_tree, FUN=prune.tree)
oj_cv_tree
```

(g) Tree with size 6 has the lowest deviation
```{r, message=FALSE}
attach(oj_cv_tree)

plot(size, dev, type='b')
```

(h) See (g)
(i)
```{r}
oj_prune <- prune.tree(oj_tree, best=6)

summary(oj_prune)

plot(oj_prune)
text(oj_prune, pretty=0)
```

(j) The training error for the unpruned tree is: 0.1588. The training error for the pruned tree is: 0.1737. Based on the training error the unpruned tree preforms better.

(k) The test error for unpruned is: 0.197, while the test error for pruned tree is 0.222. The pruned tree has a higher test error.
```{r}
oj_ppred <- predict(oj_prune, test, type='class')

table(oj_ppred, test$Purchase)
```


























