---
title: "ch8_labs"
author: "Christopher Chan"
date: "December 19, 2018"
output:
    rmarkdown::github_document:
        pandoc_arg: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(tree)
library(ISLR)
library(randomForest)
```

MSE function
```{r}
MSE <- function(x,y){
    mean((x - y)^2)
}
```


##8.3.1
Preview Carseats data.
```{r}
head(Carseats)
attach(Carseats)
```

Create predictor High and add to Carseats. Needed to set the created predictor High to a factor, originally it was created as a chr
```{r}
High <- as.factor(ifelse(Sales<=8, 'No', 'Yes'))

Carseats_high <- Carseats %>%
    mutate(High)
head(Carseats_high)
```

```{r}
dim(Carseats_high)
carseats_tree <- tree(High~.-Sales, Carseats_high)
summary(carseats_tree)
```

```{r}
plot(carseats_tree)
text(carseats_tree, cex=0.75, pretty=0)
```




```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats_test <- Carseats_high[-train,]
dim(Carseats_test)
High_test <- High[-train]

carseats_tree <- tree(High~.-Sales, Carseats_high, subset=train)
tree_pred <- predict(carseats_tree, Carseats_test, type='class')
table(tree_pred, High_test)

(87+59)/200 #Accuracy
```

```{r}
set.seed(3)
cv_carseats <- cv.tree(carseats_tree, FUN=prune.misclass)
names(cv_carseats)
cv_carseats
```

```{r}
par(mfrow=c(1,2))
plot(cv_carseats$size, cv_carseats$dev, type='b')
plot(cv_carseats$k, cv_carseats$dev, type='b')
```

```{r}
prune_carseats <- prune.misclass(carseats_tree, best = 9)
plot(prune_carseats)
text(prune_carseats, pretty=0)
```

```{r}
prune_pred <- predict(prune_carseats, Carseats_test, type='class')
table(prune_pred, High_test)

(101+74)/200 #accuracy
```

##8.3.2
```{r, message=FALSE}
library(MASS)
```

```{r}
set.seed(1)

train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree_bos <- tree(medv~., Boston, subset=train)
summary(tree_bos)
```

```{r}
plot(tree_bos)
text(tree_bos, pretty=0, cex=0.75)
```

```{r}
cv_bos <- cv.tree(tree_bos)
plot(cv_bos$size, cv_bos$dev, type='b')
```

```{r}
prune_bos <- prune.tree(tree_bos, best=5)
plot(prune_bos)
text(prune_bos, pretty=0)
```

```{r}
yhat <- predict(tree_bos, newdata=Boston[-train,])
test_bos <- Boston[-train, 'medv'] 

plot(yhat, test_bos)
abline(0, 1)
mean((yhat-test_bos)^2)
```


##8.3.3
The randomForest() can perform both bagging and random forest. By setting $m=p$ we prevent the random forest from occuring, instead only bagging occurs. This is what happens in the following code, mtry=13, so all the predictor variables are considered for each node.
```{r}
set.seed(1)

bag_bos <- randomForest(medv~., Boston, subset=train, mtry=13, importance=T)
bag_bos
```
As a side note, ntree sets the number of trees to be used in the model. By default it is set to 500.

Accuracy of bagging model.
```{r}
yhat_bag <- predict(bag_bos, newdata=Boston[-train,])

plot(yhat_bag, test_bos)
abline(0,1)

mean((yhat_bag - test_bos)^2)
```

This model will use random forest and bagging. We set mtry=6, so roughly half the predictor variables are considered.
```{r}
set.seed(1)

rf_bos <- randomForest(medv~., Boston, subset=train, mtry=6, importance=T)
rf_bos

yhat_rf <- predict(rf_bos, newdata=Boston[-train,])
MSE(yhat_rf, test_bos)
```
We get a training MSE of 11.89, slightly higher than the bagging training MSE of 11.16. But the real test will be in the test MSE. And indeed, the test MSE of the random forest + bagging is lower, at 11.66.

```{r}
importance(rf_bos)

varImpPlot(rf_bos)
```
































