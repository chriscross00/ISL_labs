---
title: "ch6_exercises"
author: "Christopher Chan"
date: "December 23, 2018"
output: 
    rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(glmnet)
library(leaps)
library(MASS)
library(pls)
library(ISLR)
```

##Conceptual
###1
(a) Best subset should have the lowest RSS because it compares all possible models, while stepwise regression only compares paraments based on the past parameters.
(b) It depends, but most likely best subset should have the lowest RSS.
(c)
    i. True
    ii. True
    iii. False
    iv. False
    v. False
    
###2
(a) iii. Lasso greatly decreases variance at the cost of increased biases when compared to LSR.  
(b) iii. Same as lasso
(c) ii. Answer

###3
(a) iv. Increasing s from 0 will cause $\beta$ to increase from 0 to their least squares estimate value.
(b) ii. While lasso provides a decreased variance, it increases bias. once $\beta$ reaches their LR values they overfit the test data.
(c) iii. Variance increases you include more data into the model.
(d) iv. Lasso insures that LR estimate values are reached.
(e) v. By definition irreducible error is model independent.

###4 
(a) iii. Increasing $\theta$ will increase the RSS because when $\theta = 0$ the $\beta$ coefficients are at their OSL estimated values, so increasing $\theta$ will decrease the $\beta$ coefficients, increasing the RSS.
(b) ii. With $\theta = 0$ $\beta$ coefficients are at their OSL estimated values, meaning they are overfitted to the training data. They have a bias towards the training data, so the test RSS will be high. As $\theta$ increases the bias decreases, so test RSS decreases. However, as some $\beta$ reach zero the model becomes too simple and test RSS increases.
(c) iv. High variance at $\theta = 0$ and decreases as parameters are removed.
(d) iii. Very low bias at $\theta = 0$ and as $\beta$s are removed the model fits the training data less, so bias increases.
(e) v. By definition irreducible error is model independent.

##Applied
###8
(a) 
```{r}
set.seed(1)

x <- rnorm(100)
noise <- rnorm(100)
```

(b)
```{r}
b_0 <- 17
b_1 <- 3
b_2 <- -0.5
b_3 <- -3

y = b_0 + b_1*x + b_2*x^2 + b_3*x^3 + noise
```

(c)
```{r}
df <- tibble(x, y)

poly_x <- regsubsets(y~poly(x, 10, raw = T), data=df, nvmax=10)
summary_poly_x <- summary(poly_x)
```

Best subset - optimal polynomial is 4
```{r}
which.min(summary_poly_x$cp)
which.min(summary_poly_x$bic)
which.min(summary_poly_x$adjr2)

coef(poly_x, id = 3)

par(mfrow=c(2,2))
plot(summary_poly_x$rss, xlab='# variables', ylab='RSS', type='l')
plot(summary_poly_x$adjr2, xlab='# variables', ylab='Adjusted RSq', type='l')
plot(summary_poly_x$cp, xlab='# variables', ylab='Cp', type='l')
plot(summary_poly_x$bic, xlab='# variables', ylab='bic', type='l')
```

(d)
Forward - optimal polynomial is 5
```{r}
poly_fwd <- regsubsets(y~poly(x, 10, raw=T), df, nvmax=10, method='forward')
summary_poly_fwd <- summary(poly_fwd)

which.min(summary_poly_fwd$cp)
which.min(summary_poly_fwd$bic)
which.min(summary_poly_fwd$adjr2)
```

Backward - optimal polynomial is 3
```{r}
poly_bwd <- regsubsets(y~poly(x, 10, raw=T), df, nvmax=10, method='backward')
summary_poly_bwd <- summary(poly_bwd)

which.min(summary_poly_bwd$cp)
which.min(summary_poly_bwd$bic)
which.min(summary_poly_bwd$adjr2)


par(mfrow=c(2,2))
plot(summary_poly_bwd$rss, xlab='# variables', ylab='RSS', type='l')
plot(summary_poly_bwd$adjr2, xlab='# variables', ylab='Adjusted RSq', type='l')
plot(summary_poly_bwd$cp, xlab='# variables', ylab='Cp', type='l')
plot(summary_poly_bwd$bic, xlab='# variables', ylab='bic', type='l')

```

Best subset and backward stepwise regression are quite close while foward stepwise regression  is a little farther away.

(e)
```{r}
model_mat <- model.matrix(y~poly(x, 10, raw=T), df)[,-1]

lasso <- cv.glmnet(model_mat, y)


plot(lasso)
best_lmd <- lasso$lambda.min
best_lmd
```

```{r}
out <- glmnet(model_mat, y)
predict(out, type='coefficients', s=best_lmd)
```

The lasso model predicts that $X^9$ preforms the best, far better than the other models.

(f)
Generate the new response variable.
```{r}
b_7 = 7

y = b_0 + b_7*(x^7) + noise
```

Best subset regression. Again it chooses something between 3 and 4, so i'll pick 4.
```{r}
best_sub <- regsubsets(y~poly(x, 10, raw=T), df, nvmax=10)

s_best_sub <- summary(best_sub)
which.min(s_best_sub$cp)
which.min(s_best_sub$bic)
which.min(s_best_sub$adjr2)

coef(best_sub, id=4)
```

Lasso only returns 1 significant model; $X^7$. All other models have been reduced to 0. 
```{r}
lasso7 <- cv.glmnet(model_mat, y)
best_lmd <- lasso7$lambda.min
best_lmd

out <- glmnet(model_mat, y)
predict(out, type='coefficients', s=best_lmd)
```

###9
(a)
```{r}
set.seed(5)

train <- sample(1:nrow(College), nrow(College)/2)
test <- (-train)

c_train <- College[train,]
c_test <- College[test,]
```

(b)
```{r}
c_lm <- lm(Apps~., c_train)

summary(c_lm)
```
RSS
```{r}
lm_pred <- predict(c_lm, c_test)

mean((c_test[, 'Apps'] - lm_pred)^2)
```

(c)
Creating model.matrix for training and testing data. This makes it so that we can work with the datasets.
```{r}
train_mat <- model.matrix(Apps~., c_train)
test_mat <- model.matrix(Apps~., c_test)
grid <- 10^seq(4,-2, length=100)

c_rr <- cv.glmnet(train_mat, c_train[, 'Apps'], alpha=0, lambda=grid, thresh=1e-12)
best_lambda <- c_rr$lambda.min
best_lambda
```

Ridge regression has a slightly higher RSS than OSL.
```{r}
pred <- predict(c_rr, newx=test_mat, s=best_lambda)
mean((pred - c_test[,'Apps'])^2)
```

(d)
```{r}
c_lasso <- cv.glmnet(train_mat, c_train[, 'Apps'], lambda=grid, thresh=1e-12)
plot(c_lasso)
lasso_lam <- c_lasso$lambda.min
lasso_lam
```

We have a much higher RSS than OLS.
```{r}
pred_lasso <- predict(c_lasso, newx=test_mat, s=lasso_lam)
mean((pred_lasso - c_test[, 'Apps'])^2)
```

(e) Based on the MSEP plot the lowest MSEP occurs at 17 predictors. But really, the gain is minimal so I think we can safely use 4 predictors. 
```{r}
pcr_fit <- pcr(Apps~., data=c_train, scale=T, validation='CV')

validationplot(pcr_fit, val.type = 'MSEP')
```

With the number of components at 4 we get a RSS of 3660863. The RSS is relatively steady until running predict with ncomp=16 where the RSS drops to roughly half. 
```{r}
pcr_pred <- predict(pcr_fit, c_test, ncomp=4)
mean((pcr_pred - c_test[,'Apps'])^2)
```

(f)
```{r}
pls_fit <- plsr(Apps~., data=c_train, scale=T, validation='CV')
summary(pls_fit)
validationplot(pls_fit)
```

The RSS for the PLS is 1856693.
```{r}
pls_predict <- predict(pls_fit, c_test, ncomp=6)
mean((pls_predict - c_test[,'Apps'])^2)
```

(g) PCR has by far the worst test error.

###10
###11
(a) Getting a sense of the data.
```{r}
head(Boston)
summary(Boston)
```

####Lasso
Prepping the data
```{r}
x <- model.matrix(crim~., Boston)
y <- Boston$crim
```

Setting up the lambdas which we will use.
```{r}
grid <- 10^seq(10, -3, length=200)
```

```{r}
set.seed(1)

lasso <- cv.glmnet(x, y, type.measure='mse')
plot(lasso)
coef(lasso)
```


















































