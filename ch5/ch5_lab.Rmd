---
title: 'ch5_lab'
author: 'Christopher Chan'
date: 'December 31, 2018'
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(tidyverse)
library(boot)
```

##5.3.1
Setting up the training set. 392 is just a random number that ISL came up with and we are only taking half of it.
```{r}
set.seed(1)

train <- sample(392, 196)
test <- slice(Auto, -train)
```

```{r}
lm_fit <- lm(mpg~horsepower, Auto, subset=train)

summary(lm_fit)
```

```{r}
pred <- predict(lm_fit, test)

mean((test[,'mpg'] - pred)^2)
```

Polynomial fit with quadratic and cubic functions. Both fit quite a bit better, with cubic having the lowest RSS. 
```{r}
lm_fit_quad <- lm(mpg~poly(horsepower, 2), data=Auto, subset=train)
pred_quad <- predict(lm_fit_quad, test) 

mean((test[,'mpg'] - pred_quad)^2)
```
```{r}
lm_fit_cubic <- lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
pred_cubic <- predict(lm_fit_cubic, test)

mean((test[,'mpg'] - pred_cubic)^2)
```


##5.3.2
```{r}
glm_fit <- glm(mpg~horsepower, data=Auto)

cv_err <- cv.glm(Auto, glm_fit)
cv_err$delta
```

LOOCV for polynomial lm.
```{r}
cv_error <- rep(0,5) #initializes the cv_error vector

for (i in 1:5){
    glm_fit <- glm(mpg~poly(horsepower, i), data=Auto)
    cv_error[i] <- cv.glm(Auto, glm_fit)$delta[1]
}

cv_error
```

##5.3.3
k-fold cv with $k=10$
```{r}
set.seed(5)

cv_error_10 <- rep(0,10)
for (i in 1:10){
    glm_fit <- glm(mpg~poly(horsepower, i), data=Auto)
    cv_error_10[i] <- cv.glm(Auto, glm_fit, K=10)$delta[1]
}

cv_error_10
```

















































