---
title: "ch5_exercises"
author: "Christoper Chan"
date: "January 1, 2019"
output: 
    rmarkdown::github_document:
        pandoc_arg: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(ISLR)
library(boot)
library(MASS)
```

##Conceptual
###1
###2
(a) $(n-j)/n$ That is the probability of not $j$ is the total probability minus the probability of $j$.
(b) $(n-j)/n$ The probabilty is the same as the 1st bootstrap observation. 
(c) Bootstraping observations are independent, so the outcome of each observation does not affect the outcome of the $n$th observation. Using the product rule we simply raise the probability of $j$ not appearing to the $n$th bootstraping observation
(d) $Pr(in) = 1 - Pr(out) = 1 - (1-1/n)^n =$ `r 1-(1-1/5)^5`
(e) `r 1-(1-1/100)^100`
(f) `r 1-(1-1/10000)^10000`
(g) The probabilty drops off quite fast as the number of observations increase. Most are barely above zero.
```{r}
pr <- function(n){
    return (1 - (1-1/n)^n)
}
x <- 1:1e5

pr_x <- tibble(x, pr(x))

ggplot(pr_x, aes(x, pr(x))) +
    geom_point()
```

(h)
```{r}
store <- rep(NA, 1000)
for (i in 1:1000){
    store[i] <- sum(sample(1:100, rep=T)==4) > 0
}

mean(store)
```

###3
(a) Divides the dataset into $k$ blocks and validates the data on the $k$th block for that iteration. The blocks rotate until all $k$ blocks have been used as the validation block. The test error is averaged over all iterations.
(b)
    + More groups give a better representation of the true test error because of the larger sample size. It also has a lower variance. It will also have lower bias as it tends not to overfit to the training data.
    + Computationally it is more efficient as it does not have to iterate $n$ times. Also $k$-fold cv will have a lower bias.

###4

##Applied
###5
(a)
```{r}
set.seed(1)


summary(Default)

lg <- glm(default~balance + income, Default, family='binomial')
summary(lg)
```

(b)
```{r, message=FALSE}
val <- function() {
    train <- sample(1:nrow(Default), nrow(Default)/2)
    test <- slice(Default, -train)

    glm_fit <- glm(default~balance + income, Default, family='binomial', subset=train)

    t <- predict(glm_fit, test, type='response')
    glm_pred <- rep('No', nrow(test))
    glm_pred[t >0.5] = 'Yes'

    return (mean(glm_pred != test$default))
}


val()
```

(c) They all have low errors that are relatively similar.
```{r}
map(seq_len(3), ~val())
```

(d)

###9
(a)
```{r, message=FALSE}
attach(Boston)
summary(Boston)

medv_mn <- mean(medv)
```

(b)
```{r}
medv_err <- sd(medv)/sqrt(length(medv))
medv_err
```

```{r}

```




































