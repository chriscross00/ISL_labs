---
title: "ML Algorithms"
author: "Christopher Chan"
date: "January 23, 2019"
output: 
    rmarkdown::github_document:
        pandoc_arg: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Linear Regression
Assumptions:

1. Observations $y_i$ are uncorrelated
2. Observations $y_i$ have constant variance
3. $x_i$ are fixed

$$f(X) = \beta_0 + \sum^p_{j=1}X_j\beta_j + \varepsilon$$

Cost function
$$RSS(\beta) = \sum^N_{i=1}(y_i-\beta_0-\sum^p_{j=1}x_{ij}\beta_j)^2$$
Cost function matrix formula
$$\hat{\beta} = (X^TX)^{-1}X^Ty$$

Hat (Matrix) - predicts y
$$H = X(X^TX)^{-1}X^T$$

Variance - unbiased 
$$\hat{\sigma} = \frac{1}{N-p-1}\sum^N_{i=1}(y_i-\hat{y_i})^2$$

#### Ridge Regression

$$\hat{\beta}^{ridge} = argmin_\beta [\sum_{i=1}^N(y_i - \beta_0 -\sum^p_{j=1}x_{ij}\beta_j)^2 + \lambda\sum^p_{j=1}\beta^2_j]$$

#### Lasso Regression

$$\hat{\beta}^{lasso} = argmin_\beta [\frac{1}{2}\sum_{i=1}^N(y_i - \beta_0 -\sum^p_{j=1}x_{ij}\beta_j)^2 + \lambda\sum^p_{j=1}\mid{\beta_j}]$$


### Logistic Regresion

In general modeling:
$$p(X) = p(Y=y|X)$$



$$log\frac{p(X)}{1-p(X)} = \beta_0 + \beta_1X_1 + ... + \beta_pX_p$$

$$p(X) = \frac{e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}}$$

Estimating regression coefficients:
$$\ell(\beta_0, \beta_1 ... \beta_p) = \prod_{i:y_i=1}p(x_i) \prod_{i^{'}:y_i^{'}=0} (1-p(x_{i^{'}}))$$

Cost function:
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_{\theta}(x^{(i)})) + (11-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$$

### Naive Bayes

$$p(y|x_1, ..., x_n)\propto p(y)\prod_{i}^{n}P(x_i|y)$$
$$y = argmax_yP(y)\prod_{i}^{n}P(x_i|y)$$


### Notes
$x$ parameterized by $\theta$: $$x;\theta$$
